# BloomEval
Recent advances in Large Reasoning Models (LRMs) based on Chain-of-Thought (CoT), such as DeepSeek-R1 and OpenAI-o1, have demonstrated potential in complex reasoning tasks by simulating human System 2 thinking. However, their rigid reliance on the cautious thinking mode may induce cognitive biases like overthinking and logical gaps. While existing evaluation benchmarks (e.g., GSM8K, MATH) focus on reasoning results, they lack consideration of cognitive dimensions and fail to systematically evaluate the hierarchical thinking processes of LRMs. To fill this gap, we propose BloomEval, a novel benchmark inspired by Bloom's Taxonomy to evaluate LRMs’ thinking capability. BloomEval presents 94,602 mathematical problems from K12 to higher education with fine-grained cognitive annotations, including Bloom’s cognitive levels, problem parameters, and cognitive level-aligned solution steps. An Expert-LLM Collaborative Annotation framework is designed to ensure the consistency and reliability of annotation through a multi-stage workflow and dispute resolution mechanism. Experiments evaluate the thinking ability of mainstream LRMs from the perspective of cognitive hierarchy coverage, logic coherence and higher-order thinking. Our findings reveal that LRMs suffer from overthinking, incomplete thinking processes, and cognitive hierarchy jumps, especially when solving high-order thinking problems. Our work provides empirical foundations for diagnosing cognitive bottlenecks and optimizing reasoning paths of LRMs.
